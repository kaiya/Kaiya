<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="renderer" content="webkit">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <title>Survey on Security and Privacy of Machine Learning - Kaiya Xiong&#39;s Blog</title>
  
  <meta name="description" content="Survey on Security and Privacy of Machine Learning  TOC {:toc}
Abstract  ML was used in image processing, natural language processing, pattern recognition, cybersecurity and other fields.
  And Scenarios: facial recognition, malware detection, automatic driving, and intrusion detection. But these algorithms and corresponding training data are vulnerable.
 Academia and industry have found out many security threats against a variety of learning algorithms, including naive Bayes, logistic regression, decision tree, support vector machine (SVM), principle component analysis, clustering, and prevailing deep neural networks.">
  <meta name="author" content="">
  
  <link href="https://fonts.loli.net/css?family=PT+Sans:400,400i,700,700i" rel="stylesheet">
  <link href="https://cdn.bootcss.com/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet">
  <link href="https://kaiyai.com/css/style.css" rel="stylesheet">
  
  <link rel="apple-touch-icon" href="https://kaiyai.com/img/apple-touch-icon.png">
  <link rel="icon" href="https://kaiyai.com/img/favicon.ico">
  
  <meta name="generator" content="Hugo 0.49">
  
  <link rel="alternate" type="application/atom+xml" href="https://kaiyai.com/index.xml" title="Kaiya Xiong&#39;s Blog">
</head>
<body class="single">
  <header class="header">
    
    <p class="title"><a href="https://kaiyai.com/">Kaiya Xiong&#39;s Blog</a></p>
    
    <button class="menu-toggle" type="button"></button>
    <nav class="menu">
      <ul>
        
        
        <li class="">
          <a href="/about/">About</a>
        </li>
        
      </ul>
    </nav>
  </header>
  <main class="main">

<article class="post post-view">
  <header class="post-header">
    <h1 class="post-title">Survey on Security and Privacy of Machine Learning</h1>
    <p class="post-meta">2018.7.18</p>
  </header>
  <div class="post-content">

<h2 id="survey-on-security-and-privacy-of-machine-learning">Survey on Security and Privacy of Machine Learning</h2>

<ul>
<li><p>TOC
{:toc}</p>

<h3 id="abstract">Abstract</h3>

<blockquote>
<p>ML was used in image processing, natural language processing, pattern recognition, cybersecurity and other fields.</p>
</blockquote></li>
</ul>

<p>And Scenarios: facial recognition, malware detection, automatic driving, and intrusion detection.
But these algorithms and corresponding training data are vulnerable.</p>

<blockquote>
<p>Academia and industry have found out many security threats against a variety of learning algorithms, including naive Bayes, logistic regression, decision tree, support vector machine (SVM), principle component analysis, clustering, and prevailing deep neural networks.</p>
</blockquote>

<p>Survey from two aspects: the training phase and the testing/inferring phase.</p>

<p>defensive techniques, four groups:</p>

<ul>
<li>security assessment mechanisms</li>
<li>countermeasures in the training phase</li>
<li>countermeasures in the testing or inferring phase</li>
<li>data security, and privacy</li>
</ul>

<h3 id="1-introduction">1. Introduction</h3>

<h3 id="2-basic-concept-model-taxonomy">2. Basic Concept, Model, Taxonomy</h3>

<h4 id="1-basic-of-machine-learning">1. Basic of Machine Learning</h4>

<p>According to differences of feedbacks, machine learning related works can be categorized into three groups:
* supervised learning: decision tree, support vector machine (SVM), neural networks, etc.
* unsupervised learning: Clustering and auto-encoder.
* reinforcement learning: optimizes behavior strategies via try-and-error.</p>

<h4 id="2-adversarial-model">2. Adversarial Model</h4>

<p>Biggio <em>et al.</em> argued that well-deÔ¨Åned adversarial model should be constructed with four dimensions, goal, knowledge, capability and attacking strategy.</p>

<ol>
<li>Goal: can be clearly described using both the expected impacts and the attack speciÔ¨Åcity of security threats.</li>
<li>Knowlodge: divided into two groups named <strong>constrained knowledge</strong> and <strong>complete knowledge</strong> by examining whether or not an attacker know training data, features, learning algorithms, decision functions, classiÔ¨Åer parameters and feedback information.</li>
<li>Capability: qualitative interpreted from three aspects: (1) Is the impact of security threats causative or exploratory? (2) What is the percentage of training and testing data that are controlled by the attacker? (3) What is the extent of features and parameters that are known by the attacker?</li>
<li>Attacking strategy: speciÔ¨Åc behaviors of manipulating training and testing data to effectively achieve his/her goals.</li>
</ol>

<h4 id="3-security-threats-taxonomy">3. Security threats taxonomy</h4>

<h5 id="1-influence-on-classifiers">1. Influence on classifiers</h5>

<ul>
<li>Causative Attack: adversaries change the distri of training data, which induces param chagnes of learning models when retraining.</li>
<li>Exploratory attack: does not modify already trained classifiers, but aims to cause misclassification with respect to adversarial samples or to uncover sensitive info.</li>
</ul>

<h5 id="2-security-violation">2. Security violation</h5>

<ul>
<li>Integrity attack: try to increase the <strong>false negatives</strong> of existing classifiers when classifying harmful samples.</li>
<li>Availability attack: on the contrary, increase the <strong>false positives</strong> of classifiers with respect to benign samples.</li>
<li>Privacy violation attack: able to obtain sensitive and confidential info.</li>
</ul>

<h5 id="3-attack-specificity">3. Attack specificity</h5>

<ul>
<li>Targeted attack: highly directed to reduce the performance of classifiers on one particular sample or one specific group of samples.</li>
<li>Indiscriminate attack: causes the classifiers to fail in an indiscriminate fashion on a broad range of samples.</li>
</ul>

<p><img src="https://lib.azfs.com.cn/2018-07-15-084523.png" alt="taxonomy of security threats" /></p>

<h3 id="3-security-threats-towards-machine-learning">3. Security Threats towards machine learning</h3>

<p>An attacker can inject malicious and designated data into training data during the training procedure of machine learning based intrusion detection systems, inducing a significant decrease of the performance of these systems.</p>

<p><strong>Clustering</strong>  is a kind of unsupervised learning method, which can discover implicit patterns of data distributions. Most of attacks against clustering algorithms reduce their accuracy by injecting malicious data. <strong>Obfuscation attack</strong> is another type of threat that compromises clustering algorithms. It&rsquo;s goal against the targeted cluster is to generate a blend of adversarial samples and normal ones from other clusters without altering the clustering results of these normal samples, resulting in a set of stealthy adversarial samples.</p>

<p><strong>Deep Learning</strong> : as a typical arch of dl, DNN is demonstrated to be effective in various pattern recognition tasks. DNN is also vulnerable to various adversarial attacks. In image classification, DNN only extracts a small set of features, resulting in poor performance on the images with minor differences. Potential adversaries can exploit such vulnerability to evade anomaly detection. In 2013, Szegedy <em>et al.</em> proposed to use the generated image with slight turbulence to deceive the pre-trained DNN.</p>

<h4 id="1-training-phase">1. Training Phase</h4>

<p><strong>Poisoning attack</strong> : a type of causative attack, disrupts availability and integrity of models via injecting adversarial samples to the training data. Such adversarial samples are designated to have similar features with malicious samples but incorrect labels, including change of training data distribution. It&rsquo;s not easy to alter the data itself. So the adversaries exploit the vulnerability that stems from <strong>retraining</strong> existing machine learning models. Machine learning based systems are generally required to <strong>periodically update</strong> their decision models to adapt to varying application contexts. So the adversaries can utilize the periodic update characteristic.</p>

<p>For unsupervised learning, e.g., clustering analysis, it&rsquo;s not applicable for changing the sample labels. However, some researches introduced how to launch poisoning attack against single-linkage and complete-linkage hierarchical clustering. For example, a heuristic strategy can be adopted to measure the impact induced by adversarial samples on clustering accuracy via introducing a <em>bridge</em> concept. Then the optimal adversarial samples were selected to effectively reduce the clustering accuracy. Three commonly used graph clustering or embedding techniques:</p>

<ul>
<li>Community discovery</li>
<li>Singular value decomposition (SVD)</li>
<li>Node2vec</li>
</ul>

<p>There are two novel attacks, named <strong>targeted noise injection</strong> and <strong>small community</strong>. These attacks can effectively evade graph clustering approaches with limit adversaries knowledge and low cost. Two defense strategies: <strong>Training Classifier with Noise</strong> (similar to adversarial training) and <strong>Improving hyper-parameter selection</strong> , but the defense effect is not significant.</p>

<h5 id="1-poisoning-without-modifying-features-or-labels">1) Poisoning without modifying features or labels</h5>

<p>adversarial samples selecting methods:</p>

<ul>
<li>poisoning models: selected according to the degree of performance reduction in terms of the classification accuracy of learning models over validating data sets. (Feasible on decision tree, nearest neighbor classifier, multilayer perception, and SVM)</li>
<li><strong>gradient ascent strategy</strong>: selected by calculating the gradient of objective functions that measure the effectiveness of adversarial samples. (Feasible on SVM, LASSO, PDF malware detection systems)</li>
<li><strong>GAN</strong>: generative model is trained to generate candidate adversarial samples. Then, the discriminative model is used to select the optimal samples with a specific loss function.</li>
</ul>

<p>Comparative results between GAN and direct gradient methods on MNIST and CIFAR-10 data sets validated that GAN was able to rapidly generate high-quality adversarial samples with a large loss value.</p>

<h5 id="2-poisoning-with-modifying-features-or-lables">2) Poisoning with modifying features or lables</h5>

<p>More powerful adversary model has the capability of modifying extracted features or the labels of some training data.</p>

<blockquote>
<p>Transferring the problem of selecting target labels to an optimization one</p>
</blockquote>

<ul>
<li>In <strong>label contamination attack</strong>(LCA), an attacker can significantly reduce the performance of SVM by <strong>flipping the labels of some training data</strong>.</li>
<li>Xiao. <em>et al.</em> Extended the model to attack against some black-box linear learning such as SVM and logistic regression.</li>
</ul>

<h4 id="2-testing-or-inferring-phase">2. Testing or Inferring Phase</h4>

<p>Utilizing the trained model to classifying or clustering new data. The most common types of security threats against the testing/Inferring phase include <code>spoofing</code> (for example, <code>evasion</code> and <code>impersonate</code> threats) and <code>Inversion attack</code>.</p>

<p><strong>1) Evasion attack</strong>: main idea is that an attacker generates some adversarial samples that are able to evade detection such that overall security of target systems is significantly reduced.</p>

<ul>
<li>Attack and Defense: generate the optimal adversarial samples to evade detection via gradient algorithms.</li>
<li>Evasion attacks was feasible for use to attack against FRS and malware detection real world, resulting in severe security threats towards target systems.</li>
</ul>

<p><strong>2) Impersonate attack</strong>: <strong>imitate data samples from victims</strong>, use in image recognition, malware detection. Attacker aims to generate specific adversarial samples such that existing machine learning-based systems wrongly classify the original samples with different labels from the impersonated ones. Such attack is <strong>particularly effective in attacking DNN algorithms</strong> because <em>DNN usually extracts a small feature set to facilitate the object identification</em>. So, attacker can easily launch impersonate attacks by <strong>modifying some key features</strong>.</p>

<ul>
<li>Electrical world: Use a revised genetic algorithm called Multi-dimensional Archive of Phenotypic Elites(MAP-Elites), to generate the optimal adversarial samples after evolving images from different categories. Then, these samples were fed into AlexNet and Le-Net-5 network, resulting in the performance reduction of DNNs.</li>
<li>Physical world: adversary generated electronic adversarial samples via the least likely class method. Then these adversarial images were printed out to serve as the input of camera. Due to the feature loss during printing and photography, the successful rate was much lower than that in electrical world. But it also validated the feasibility of impersonate attacks in real world.</li>
</ul>

<p>More work show that <strong><em><code>transferable</code></em></strong> adversarial samples could be generated from <code>ensemble learning</code>, where the output samples from one DNN were effective for use to attack against other DNNs.</p>

<p><em>Hidden voice commands</em> :</p>

<blockquote>
<p>Real experiments: the voice with no meanings in the perspective of human-beings could be used to emulate real voice control commands.</p>
</blockquote>

<p><strong>3) Inversion attack</strong> : utilizes the API provided by existing machine learning systems to gather some basic information regarding target system models. Then, the basic information is fed into <strong>reverse analysis</strong> followed by the leakage of privacy data embedded in target models.</p>

<p>According to the degree of understanding knowledge in adversarial models, this type of attack can be generally classified into two groups: namely <strong>white-box attack</strong> and <strong>black-box attack</strong>.</p>

<h4 id="3-summary-of-adversarial-attacks-against-machine-learning">3. Summary of Adversarial attacks against machine learning</h4>

<p><img src="https://lib.azfs.com.cn/2018-07-15-144336.png" alt="image-20180715224335752" /></p>

<h3 id="4-defensive-techniques-of-machine-learning">4. Defensive Techniques of Machine Learning</h3>

<p><img src="https://lib.azfs.com.cn/2018-07-15-155824.png" alt="image-20180715235824018" /></p>

<h4 id="1-security-assessment-mechanisms">1. Security assessment mechanisms</h4>

<p>Most of existing assessing techniques focus on quantitatively evaluating the performance of various learning algorithms rather than their security.
A designer first introduces adversarial assumptions towards classifer vulnerabilities. Then, the designer proposes countermeasures to protect classifiers from the adversaries. There are two types of defensive mechanisms, i.e., <strong>proactive defense</strong> and <strong>reactive defense</strong>, as illustrated in Fig. 4.
<img src="https://lib.azfs.com.cn/2018-07-15-155800.png" alt="image-20180715235759999" /></p>

<p>The notable differences between proactive and reactive defending mechanisms include the following two aspects:</p>

<ul>
<li>The attacking and defending subjects are both the classifier designer in proactive defense.</li>
<li>The designer only performs penetration testing to uncover vulnerabilities rather than a true attack against the classifier. In other words, penetration testing in the proactive defending mechanism and attacking in the reactive one are benign and malicious, respectively.</li>
</ul>

<p>The distribution of training data and that of testing data will be notably different with the presence of adversarial samples. Such abnormal phenomenon can be used to serve as a way of assessing the security of machine learning and to predict whether or not the adversarial samples exists.</p>

<p>So, some researchers proposed quantitative security analysis and evaluation of ml algorithms in a adversarial env.
<img src="https://lib.azfs.com.cn/2018-07-15-162410.png" alt="image-20180716002410135" />
Fig. 5 illustrates an example of proactive security assessment considering data distributions.</p>

<p>First, select proper adversarial models with respect to the hypothesized attack scenario defined at the conceptual level by making assumptions on the goal, knowledge, capacity and corresponding strategy.</p>

<p>Then, it defines the distributions p(Y), p(A|Y) and p(<strong>X</strong>|Y, A) for training and testing data, where Y belongs to set {L, M} and A belongs to set {F, T} respectively refer to class labels (L:legitimate; M:malicious) and a Boolean random variable representing whether or not a given sample has been manipulated (A=T) or not(A=F).</p>

<p>After that, it constructs sample training <strong>TR</strong> and testing <strong>TS</strong> sets according to the data model defined before, given k&gt;=1 Paris of data sets $(D<em>{TR}^I, D</em>{TS}^i), i=1,&hellip;,k$ that are obtained from classical <code>resampling</code> techniques, e.g., <code>cross-validation</code> or <code>bootstrapping</code>. Finally, the classifier performance with the presence of simulated attack is evaluated using the constructed(TR^i, TS^i) pairs.</p>

<h4 id="2-countermeasures-in-training-phase">2. Countermeasures in Training phase</h4>

<p>Two main countermeasures:</p>

<ul>
<li>Ensuing the purity of training data.</li>
<li>Improving the robustness of learning algorithms.</li>
</ul>

<p><strong>Data Sanitization</strong> : ensuring the purity of training data by separating adversarial samples from normal ones, and then removing these malicious samples. E.g., Reject on Negative Impact(RONI) defense tested the impact of each email in the training phase and did not train on message that had a large negative impact. To quantitatively measure impacts on the classification performance, the method <strong>compare error rate between the original classifier and the new one</strong>, which was retrained after adding new samples into the original training data, over the same testing data. If the error rate of the new classifier was much lower than that of the original one, then the new added samples were considered as malicious data and would be removed from training data.</p>

<p><strong>Improving the robustness of learning algorithms</strong> is another feasible defending technique, e.g., Bootstrap Aggregating and Random Subspace Method(RSM).</p>

<p><strong>Design secure learning algorithms</strong>, for example, Demontis <em>et al.</em> proposed a defending method that improved the security of linear classifier by learning more evenly-distributed feature weights.</p>

<h4 id="3-countermeasures-in-testing-inferring-phase">3. Countermeasures in Testing/Inferring phase</h4>

<p>Focus on the improvement of learning algorithms&rsquo; robustness.</p>

<p><strong>Game theory</strong></p>

<p>Tho <em>et al.</em> proposed invariant SVM algorithms using the min-max method to address the worst case feature manipulation activities in the testing phase.</p>

<p>To improve the robustness, Scheffer <em>et al.</em> proposed Stackelberg Games for adversarial prediction problems and a nashSVM algorithm based on the Nash equilibrium.</p>

<p>Bulo <em>et al.</em> extended previous work and proposed a randomized prediction game by considering randomized strategy selections according to some probability distribution defined over the respective strategy set. Results show that this method could improve the trade-off between attack detection and false alarms of classifiers.</p>

<p><strong>Active defense considering data distribution</strong></p>

<p>The goal of adversarial samples in the testing/Inferring phase is to alter data distribution of test data. So, a feasible way of defending against adversaries is to <strong>fit the testing data distribution by <code>retraining</code> learning models</strong> by classifier designers with adversarial samples. So that the new trained classifier are able to detect anomalies in the testing phase.</p>

<p><strong>Smoothing model output</strong></p>

<p>Is also effective to strengthen the robustness of learning models.</p>

<p>To protect deep learning algorithms in adversarial settings, <strong><code>defensive distillation</code></strong> was proposed to defend against adversarial samples on DNNs. Papernot <em>et al.</em> analyzed the generalization and robustness properties of defensive distillation. Comparative results validated that the technique could effectively enhance the performance of two DNNs with different architectures to detect adversarial samples in terms of the success rate of adversarial sample crafting from 95.86%(87.89%) to 0.45%(5.11%) on the MINST(CIFAT-10) data set.</p>

<p><strong><code>Dimension reduction</code></strong>: can be used to protect machine learning models from evasion attacks. This strategy aimed to enhance the resilience of classifiers by reducing the dimension of sample features.</p>

<h4 id="4-data-security-and-privacy">4. Data security and privacy</h4>

<ul>
<li><strong>Differential Privacy</strong></li>
<li>Homomorphic encryption</li>
<li>Multi-party computation</li>
<li>K-means clustering algorithms</li>
</ul>

<h3 id="5-challenges-and-future-opportunities">5. Challenges and future opportunities</h3>

<p>Five notable trends on security threats and defensive techniques of machine learning</p>

<ol>
<li>New security threats towards machine learning are constantly emerging.</li>
<li>Security assessment on machine learning based decision systems in adversarial environments becomes a prevailing research area.</li>
<li>Data privacy plays an important role in protecting the security of machine learning.</li>
<li>Secure deep learning is a new growth point in the Ô¨Åeld of machine learning security.</li>
<li>Jointly optimizing security, generalization performance and overhead is required to design secure learning algorithms.</li>
</ol>
</div>
  <footer class="post-footer">
    
  </footer>
  
  
  
  
</article>
</main>
<footer class="footer">
  <span>&copy; 2018 Kaiya Xiong&#39;s Blog üéâ</span>
  <span>&middot;</span>
  <span>Powered by <a href="https://gohugo.io/" target="_blank">HugoÔ∏èÔ∏è</a> ‚ù§Ô∏è</span>
</footer>
<script src="https://cdn.bootcss.com/instantclick/3.0.1/instantclick.min.js" data-no-instant></script>
<script data-no-instant>InstantClick.init();</script>
<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js" data-no-instant></script>

  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
<script data-no-instant>
  hljs.initHighlightingOnLoad();
  addMenuListener();
  InstantClick.on('change', function() {
    var blocks = document.querySelectorAll('pre code');
    for (var i = 0; i < blocks.length; i++) {
      hljs.highlightBlock(blocks[i]);
    }
    addMenuListener();
    if (typeof MathJax !== 'undefined') 
      MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
  });
  function addMenuListener() {
    var $toggle = document.querySelector('.menu-toggle');
    var $body = document.querySelector('body');
    $toggle.addEventListener('click', function() {
      $body.classList.toggle('noscroll');
    }, false);
  }
</script>
</body>
</html>

